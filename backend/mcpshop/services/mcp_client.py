import os
import sys
import json
import asyncio
from dotenv import load_dotenv
from fastmcp import Client
from openai import OpenAI

# 1. è½½å…¥ .env
load_dotenv(r"C:\CodeProject\Pycharm\MCPshop\.env")


class MCPClient:
    """åŸºäº HTTP çš„ MCP demo å®¢æˆ·ç«¯"""

    def __init__(self, server_url: str):
        api_key = os.getenv("OPENAI_API_KEY")
        if not api_key:
            raise ValueError("âŒ è¯·åœ¨ .env ä¸­è®¾ç½® OPENAI_API_KEY")

        # OpenAI åŒæ­¥ SDKï¼ˆåŒ…è£…åˆ°çº¿ç¨‹æ± é‡Œï¼‰
        self.oa = OpenAI(api_key=api_key, base_url=os.getenv("BASE_URL") or None)
        self.model = os.getenv("MODEL", "deepseek-chat")

        # fastmcp HTTP å®¢æˆ·ç«¯
        self.client = Client(server_url)

    # ------------------------- æ ¸å¿ƒé€»è¾‘ -------------------------

    async def process_query(self, query: str) -> str:
        """å‘ LLM å‘é€æ¶ˆæ¯ï¼Œå¿…è¦æ—¶è‡ªåŠ¨è°ƒç”¨ MCP å·¥å…·"""
        messages = [{"role": "user", "content": query}]

        # â‘  å‘æœåŠ¡å™¨æ‹‰å–å…¨éƒ¨å·¥å…· schema
        tools = await self.client.list_tools()
        func_schemas = [
            {
                "type": "function",
                "function": {
                    "name": tool.name,
                    "description": tool.description,
                    "parameters": getattr(tool, "inputSchema", getattr(tool, "input_schema", {})),
                },
            }
            for tool in tools
        ]

        # â‘¡ é¦–è½®æ¨ç†ï¼ˆå¯èƒ½è§¦å‘ tool_callsï¼‰
        first = await asyncio.to_thread(
            self.oa.chat.completions.create,
            model=self.model,
            messages=messages,
            tools=func_schemas,
        )
        choice = first.choices[0]

        # æ—  tool_callï¼šç›´æ¥è¿”å›æ–‡æœ¬
        if choice.finish_reason != "tool_calls":
            return choice.message.content

        # â‘¢ æ‰§è¡Œä¸€æ¬¡å·¥å…·
        tc = choice.message.tool_calls[0]
        tool_name = tc.function.name
        tool_args = json.loads(tc.function.arguments)
        print(f"[è°ƒç”¨å·¥å…·] {tool_name} {tool_args}")

        exec_res = await self.client.call_tool(tool_name, tool_args)

        # fastmcp â‰¥0.4 ç›´æ¥è¿”å›åŸå§‹ç»“æœï¼›æ—§ç‰ˆè¿”å›å¸¦ .content çš„å¯¹è±¡
        result_content = getattr(exec_res, "content", exec_res)

        # â‘£ æŠŠå·¥å…·ç»“æœå†™å›å¯¹è¯ï¼Œå†æ¬¡æ¨ç†
        messages.append(choice.message.model_dump())
        messages.append(
            {
                "role": "tool",
                "tool_call_id": tc.id,
                "name": tool_name,
                # OpenAI è¦æ±‚ stringï¼Œæ‰€ä»¥å…ˆè½¬ JSON å­—ç¬¦ä¸²
                "content": json.dumps(result_content, ensure_ascii=False),
            }
        )

        second = await asyncio.to_thread(
            self.oa.chat.completions.create,
            model=self.model,
            messages=messages,
        )
        return second.choices[0].message.content

    # ------------------------- CLI å¯¹è¯å¾ªç¯ -------------------------

    async def chat_loop(self):
        print("ğŸ¤– è¿›å…¥å¯¹è¯ï¼ˆHTTP æ¨¡å¼ï¼‰ï¼Œè¾“å…¥ quit é€€å‡º")
        while True:
            prompt = input("ä½ : ").strip()
            if prompt.lower() == "quit":
                break
            try:
                resp = await self.process_query(prompt)
                print("ğŸ¤–:", resp)
            except Exception as e:
                print("âš ï¸ å‡ºé”™:", e)

    async def run(self):
        async with self.client as client:
            try:
                await client.ping()
                print("âœ… MCP Server æ¡æ‰‹æˆåŠŸï¼Œå¼€å§‹å¯¹è¯")
            except Exception as e:
                print("âŒ æ¡æ‰‹å¤±è´¥ï¼Œè¯·æ£€æŸ¥ URL æˆ–æœåŠ¡çŠ¶æ€ï¼š", e)
                return
            await self.chat_loop()


# ------------------------- å…¥å£ -------------------------

async def _main():
    if len(sys.argv) != 2:
        print("ç”¨æ³•: python -m mcpshop.services.mcp_client <http://host:port/mcp>")
        sys.exit(1)
    url = sys.argv[1]
    client = MCPClient(url)
    await client.run()


if __name__ == "__main__":
    asyncio.run(_main())
